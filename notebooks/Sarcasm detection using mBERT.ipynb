{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1b92f92-7803-4964-a638-08bf73f019da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade transformers datasets scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8d4dc40-5f05-4637-80dc-42b364d64c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sys\n",
    "#print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd19302e-b962-47bf-8a7c-edff34514635",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!{sys.executable} -m pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f59ae59a-de08-44e3-9708-ea51bce662ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    TrainingArguments is the subset of the arguments we use in our example scripts **which relate to the training loop\n",
      "    itself**.\n",
      "\n",
      "    Using [`HfArgumentParser`] we can turn this class into\n",
      "    [argparse](https://docs.python.org/3/library/argparse#module-argparse) arguments that can be specified on the\n",
      "    command line.\n",
      "\n",
      "    Parameters:\n",
      "        output_dir (`str`, *optional*, defaults to `\"trainer_output\"`):\n",
      "            The output directory where the model predictions and checkpoints will be written.\n",
      "        overwrite_output_dir (`bool`, *optional*, defaults to `False`):\n",
      "            If `True`, overwrite the content of the output directory. Use this to continue training if `output_dir`\n",
      "            points to a checkpoint directory.\n",
      "        do_train (`bool`, *optional*, defaults to `False`):\n",
      "            Whether to run training or not. This argument is not directly used by [`Trainer`], it's intended to be used\n",
      "            by your training/evaluation scripts instead. See the [example\n",
      "            scripts](https://github.com/huggingface/transformers/tree/main/examples) for more details.\n",
      "        do_eval (`bool`, *optional*):\n",
      "            Whether to run evaluation on the validation set or not. Will be set to `True` if `eval_strategy` is\n",
      "            different from `\"no\"`. This argument is not directly used by [`Trainer`], it's intended to be used by your\n",
      "            training/evaluation scripts instead. See the [example\n",
      "            scripts](https://github.com/huggingface/transformers/tree/main/examples) for more details.\n",
      "        do_predict (`bool`, *optional*, defaults to `False`):\n",
      "            Whether to run predictions on the test set or not. This argument is not directly used by [`Trainer`], it's\n",
      "            intended to be used by your training/evaluation scripts instead. See the [example\n",
      "            scripts](https://github.com/huggingface/transformers/tree/main/examples) for more details.\n",
      "        eval_strategy (`str` or [`~trainer_utils.IntervalStrategy`], *optional*, defaults to `\"no\"`):\n",
      "            The evaluation strategy to adopt during training. Possible values are:\n",
      "\n",
      "                - `\"no\"`: No evaluation is done during training.\n",
      "                - `\"steps\"`: Evaluation is done (and logged) every `eval_steps`.\n",
      "                - `\"epoch\"`: Evaluation is done at the end of each epoch.\n",
      "\n",
      "        prediction_loss_only (`bool`, *optional*, defaults to `False`):\n",
      "            When performing evaluation and generating predictions, only returns the loss.\n",
      "        per_device_train_batch_size (`int`, *optional*, defaults to 8):\n",
      "            The batch size *per device*. The **global batch size** is computed as:\n",
      "            `per_device_train_batch_size * number_of_devices` in multi-GPU or distributed setups.\n",
      "        per_device_eval_batch_size (`int`, *optional*, defaults to 8):\n",
      "            The batch size per device accelerator core/CPU for evaluation.\n",
      "        gradient_accumulation_steps (`int`, *optional*, defaults to 1):\n",
      "            Number of updates steps to accumulate the gradients for, before performing a backward/update pass.\n",
      "\n",
      "            <Tip warning={true}>\n",
      "\n",
      "            When using gradient accumulation, one step is counted as one step with backward pass. Therefore, logging,\n",
      "            evaluation, save will be conducted every `gradient_accumulation_steps * xxx_step` training examples.\n",
      "\n",
      "            </Tip>\n",
      "\n",
      "        eval_accumulation_steps (`int`, *optional*):\n",
      "            Number of predictions steps to accumulate the output tensors for, before moving the results to the CPU. If\n",
      "            left unset, the whole predictions are accumulated on the device accelerator before being moved to the CPU (faster but\n",
      "            requires more memory).\n",
      "        eval_delay (`float`, *optional*):\n",
      "            Number of epochs or steps to wait for before the first evaluation can be performed, depending on the\n",
      "            eval_strategy.\n",
      "        torch_empty_cache_steps (`int`, *optional*):\n",
      "            Number of steps to wait before calling `torch.<device>.empty_cache()`. If left unset or set to None, cache will not be emptied.\n",
      "\n",
      "            <Tip>\n",
      "\n",
      "            This can help avoid CUDA out-of-memory errors by lowering peak VRAM usage at a cost of about [10% slower performance](https://github.com/huggingface/transformers/issues/31372).\n",
      "\n",
      "            </Tip>\n",
      "\n",
      "        learning_rate (`float`, *optional*, defaults to 5e-5):\n",
      "            The initial learning rate for [`AdamW`] optimizer.\n",
      "        weight_decay (`float`, *optional*, defaults to 0):\n",
      "            The weight decay to apply (if not zero) to all layers except all bias and LayerNorm weights in [`AdamW`]\n",
      "            optimizer.\n",
      "        adam_beta1 (`float`, *optional*, defaults to 0.9):\n",
      "            The beta1 hyperparameter for the [`AdamW`] optimizer.\n",
      "        adam_beta2 (`float`, *optional*, defaults to 0.999):\n",
      "            The beta2 hyperparameter for the [`AdamW`] optimizer.\n",
      "        adam_epsilon (`float`, *optional*, defaults to 1e-8):\n",
      "            The epsilon hyperparameter for the [`AdamW`] optimizer.\n",
      "        max_grad_norm (`float`, *optional*, defaults to 1.0):\n",
      "            Maximum gradient norm (for gradient clipping).\n",
      "        num_train_epochs(`float`, *optional*, defaults to 3.0):\n",
      "            Total number of training epochs to perform (if not an integer, will perform the decimal part percents of\n",
      "            the last epoch before stopping training).\n",
      "        max_steps (`int`, *optional*, defaults to -1):\n",
      "            If set to a positive number, the total number of training steps to perform. Overrides `num_train_epochs`.\n",
      "            For a finite dataset, training is reiterated through the dataset (if all data is exhausted) until\n",
      "            `max_steps` is reached.\n",
      "        lr_scheduler_type (`str` or [`SchedulerType`], *optional*, defaults to `\"linear\"`):\n",
      "            The scheduler type to use. See the documentation of [`SchedulerType`] for all possible values.\n",
      "        lr_scheduler_kwargs ('dict', *optional*, defaults to {}):\n",
      "            The extra arguments for the lr_scheduler. See the documentation of each scheduler for possible values.\n",
      "        warmup_ratio (`float`, *optional*, defaults to 0.0):\n",
      "            Ratio of total training steps used for a linear warmup from 0 to `learning_rate`.\n",
      "        warmup_steps (`int`, *optional*, defaults to 0):\n",
      "            Number of steps used for a linear warmup from 0 to `learning_rate`. Overrides any effect of `warmup_ratio`.\n",
      "        log_level (`str`, *optional*, defaults to `passive`):\n",
      "            Logger log level to use on the main process. Possible choices are the log levels as strings: 'debug',\n",
      "            'info', 'warning', 'error' and 'critical', plus a 'passive' level which doesn't set anything and keeps the\n",
      "            current log level for the Transformers library (which will be `\"warning\"` by default).\n",
      "        log_level_replica (`str`, *optional*, defaults to `\"warning\"`):\n",
      "            Logger log level to use on replicas. Same choices as `log_level`\"\n",
      "        log_on_each_node (`bool`, *optional*, defaults to `True`):\n",
      "            In multinode distributed training, whether to log using `log_level` once per node, or only on the main\n",
      "            node.\n",
      "        logging_dir (`str`, *optional*):\n",
      "            [TensorBoard](https://www.tensorflow.org/tensorboard) log directory. Will default to\n",
      "            *output_dir/runs/**CURRENT_DATETIME_HOSTNAME***.\n",
      "        logging_strategy (`str` or [`~trainer_utils.IntervalStrategy`], *optional*, defaults to `\"steps\"`):\n",
      "            The logging strategy to adopt during training. Possible values are:\n",
      "\n",
      "                - `\"no\"`: No logging is done during training.\n",
      "                - `\"epoch\"`: Logging is done at the end of each epoch.\n",
      "                - `\"steps\"`: Logging is done every `logging_steps`.\n",
      "\n",
      "        logging_first_step (`bool`, *optional*, defaults to `False`):\n",
      "            Whether to log the first `global_step` or not.\n",
      "        logging_steps (`int` or `float`, *optional*, defaults to 500):\n",
      "            Number of update steps between two logs if `logging_strategy=\"steps\"`. Should be an integer or a float in\n",
      "            range `[0,1)`. If smaller than 1, will be interpreted as ratio of total training steps.\n",
      "        logging_nan_inf_filter (`bool`, *optional*, defaults to `True`):\n",
      "            Whether to filter `nan` and `inf` losses for logging. If set to `True` the loss of every step that is `nan`\n",
      "            or `inf` is filtered and the average loss of the current logging window is taken instead.\n",
      "\n",
      "            <Tip>\n",
      "\n",
      "            `logging_nan_inf_filter` only influences the logging of loss values, it does not change the behavior the\n",
      "            gradient is computed or applied to the model.\n",
      "\n",
      "            </Tip>\n",
      "\n",
      "        save_strategy (`str` or [`~trainer_utils.SaveStrategy`], *optional*, defaults to `\"steps\"`):\n",
      "            The checkpoint save strategy to adopt during training. Possible values are:\n",
      "\n",
      "                - `\"no\"`: No save is done during training.\n",
      "                - `\"epoch\"`: Save is done at the end of each epoch.\n",
      "                - `\"steps\"`: Save is done every `save_steps`.\n",
      "                - `\"best\"`: Save is done whenever a new `best_metric` is achieved.\n",
      "\n",
      "                If `\"epoch\"` or `\"steps\"` is chosen, saving will also be performed at the\n",
      "                very end of training, always.\n",
      "        save_steps (`int` or `float`, *optional*, defaults to 500):\n",
      "            Number of updates steps before two checkpoint saves if `save_strategy=\"steps\"`. Should be an integer or a\n",
      "            float in range `[0,1)`. If smaller than 1, will be interpreted as ratio of total training steps.\n",
      "        save_total_limit (`int`, *optional*):\n",
      "            If a value is passed, will limit the total amount of checkpoints. Deletes the older checkpoints in\n",
      "            `output_dir`. When `load_best_model_at_end` is enabled, the \"best\" checkpoint according to\n",
      "            `metric_for_best_model` will always be retained in addition to the most recent ones. For example, for\n",
      "            `save_total_limit=5` and `load_best_model_at_end`, the four last checkpoints will always be retained\n",
      "            alongside the best model. When `save_total_limit=1` and `load_best_model_at_end`, it is possible that two\n",
      "            checkpoints are saved: the last one and the best one (if they are different).\n",
      "        save_safetensors (`bool`, *optional*, defaults to `True`):\n",
      "            Use [safetensors](https://huggingface.co/docs/safetensors) saving and loading for state dicts instead of\n",
      "            default `torch.load` and `torch.save`.\n",
      "        save_on_each_node (`bool`, *optional*, defaults to `False`):\n",
      "            When doing multi-node distributed training, whether to save models and checkpoints on each node, or only on\n",
      "            the main one.\n",
      "\n",
      "            This should not be activated when the different nodes use the same storage as the files will be saved with\n",
      "            the same names for each node.\n",
      "        save_only_model (`bool`, *optional*, defaults to `False`):\n",
      "            When checkpointing, whether to only save the model, or also the optimizer, scheduler & rng state.\n",
      "            Note that when this is true, you won't be able to resume training from checkpoint.\n",
      "            This enables you to save storage by not storing the optimizer, scheduler & rng state.\n",
      "            You can only load the model using `from_pretrained` with this option set to `True`.\n",
      "        restore_callback_states_from_checkpoint (`bool`, *optional*, defaults to `False`):\n",
      "            Whether to restore the callback states from the checkpoint. If `True`, will override\n",
      "            callbacks passed to the `Trainer` if they exist in the checkpoint.\"\n",
      "        use_cpu (`bool`, *optional*, defaults to `False`):\n",
      "            Whether or not to use cpu. If set to False, we will use cuda or mps device if available.\n",
      "        seed (`int`, *optional*, defaults to 42):\n",
      "            Random seed that will be set at the beginning of training. To ensure reproducibility across runs, use the\n",
      "            [`~Trainer.model_init`] function to instantiate the model if it has some randomly initialized parameters.\n",
      "        data_seed (`int`, *optional*):\n",
      "            Random seed to be used with data samplers. If not set, random generators for data sampling will use the\n",
      "            same seed as `seed`. This can be used to ensure reproducibility of data sampling, independent of the model\n",
      "            seed.\n",
      "        jit_mode_eval (`bool`, *optional*, defaults to `False`):\n",
      "            Whether or not to use PyTorch jit trace for inference.\n",
      "        bf16 (`bool`, *optional*, defaults to `False`):\n",
      "            Whether to use bf16 16-bit (mixed) precision training instead of 32-bit training. Requires Ampere or higher\n",
      "            NVIDIA architecture or Intel XPU or using CPU (use_cpu) or Ascend NPU.\n",
      "        fp16 (`bool`, *optional*, defaults to `False`):\n",
      "            Whether to use fp16 16-bit (mixed) precision training instead of 32-bit training.\n",
      "        fp16_opt_level (`str`, *optional*, defaults to 'O1'):\n",
      "            For `fp16` training, Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3']. See details on\n",
      "            the [Apex documentation](https://nvidia.github.io/apex/amp).\n",
      "        fp16_backend (`str`, *optional*, defaults to `\"auto\"`):\n",
      "            This argument is deprecated. Use `half_precision_backend` instead.\n",
      "        half_precision_backend (`str`, *optional*, defaults to `\"auto\"`):\n",
      "            The backend to use for mixed precision training. Must be one of `\"auto\", \"apex\", \"cpu_amp\"`. `\"auto\"` will\n",
      "            use CPU/CUDA AMP or APEX depending on the PyTorch version detected, while the other choices will force the\n",
      "            requested backend.\n",
      "        bf16_full_eval (`bool`, *optional*, defaults to `False`):\n",
      "            Whether to use full bfloat16 evaluation instead of 32-bit. This will be faster and save memory but can harm\n",
      "            metric values.\n",
      "        fp16_full_eval (`bool`, *optional*, defaults to `False`):\n",
      "            Whether to use full float16 evaluation instead of 32-bit. This will be faster and save memory but can harm\n",
      "            metric values.\n",
      "        tf32 (`bool`, *optional*):\n",
      "            Whether to enable the TF32 mode, available in Ampere and newer GPU architectures. The default value depends\n",
      "            on PyTorch's version default of `torch.backends.cuda.matmul.allow_tf32`. For more details please refer to\n",
      "            the [TF32](https://huggingface.co/docs/transformers/perf_train_gpu_one#tf32) documentation. This is an\n",
      "            experimental API and it may change.\n",
      "        local_rank (`int`, *optional*, defaults to -1):\n",
      "            Rank of the process during distributed training.\n",
      "        ddp_backend (`str`, *optional*):\n",
      "            The backend to use for distributed training. Must be one of `\"nccl\"`, `\"mpi\"`, `\"ccl\"`, `\"gloo\"`, `\"hccl\"`.\n",
      "        tpu_num_cores (`int`, *optional*):\n",
      "            When training on TPU, the number of TPU cores (automatically passed by launcher script).\n",
      "        dataloader_drop_last (`bool`, *optional*, defaults to `False`):\n",
      "            Whether to drop the last incomplete batch (if the length of the dataset is not divisible by the batch size)\n",
      "            or not.\n",
      "        eval_steps (`int` or `float`, *optional*):\n",
      "            Number of update steps between two evaluations if `eval_strategy=\"steps\"`. Will default to the same\n",
      "            value as `logging_steps` if not set. Should be an integer or a float in range `[0,1)`. If smaller than 1,\n",
      "            will be interpreted as ratio of total training steps.\n",
      "        dataloader_num_workers (`int`, *optional*, defaults to 0):\n",
      "            Number of subprocesses to use for data loading (PyTorch only). 0 means that the data will be loaded in the\n",
      "            main process.\n",
      "        past_index (`int`, *optional*, defaults to -1):\n",
      "            Some models like [TransformerXL](../model_doc/transformerxl) or [XLNet](../model_doc/xlnet) can make use of\n",
      "            the past hidden states for their predictions. If this argument is set to a positive int, the `Trainer` will\n",
      "            use the corresponding output (usually index 2) as the past state and feed it to the model at the next\n",
      "            training step under the keyword argument `mems`.\n",
      "        run_name (`str`, *optional*, defaults to `output_dir`):\n",
      "            A descriptor for the run. Typically used for [trackio](https://github.com/gradio-app/trackio),\n",
      "            [wandb](https://www.wandb.com/), [mlflow](https://www.mlflow.org/), [comet](https://www.comet.com/site) and\n",
      "            [swanlab](https://swanlab.cn) logging. If not specified, will be the same as `output_dir`.\n",
      "        disable_tqdm (`bool`, *optional*):\n",
      "            Whether or not to disable the tqdm progress bars and table of metrics produced by\n",
      "            [`~notebook.NotebookTrainingTracker`] in Jupyter Notebooks. Will default to `True` if the logging level is\n",
      "            set to warn or lower (default), `False` otherwise.\n",
      "        remove_unused_columns (`bool`, *optional*, defaults to `True`):\n",
      "            Whether or not to automatically remove the columns unused by the model forward method.\n",
      "        label_names (`list[str]`, *optional*):\n",
      "            The list of keys in your dictionary of inputs that correspond to the labels.\n",
      "\n",
      "            Will eventually default to the list of argument names accepted by the model that contain the word \"label\",\n",
      "            except if the model used is one of the `XxxForQuestionAnswering` in which case it will also include the\n",
      "            `[\"start_positions\", \"end_positions\"]` keys.\n",
      "\n",
      "            You should only specify `label_names` if you're using custom label names or if your model's `forward` consumes multiple label tensors (e.g., extractive QA).\n",
      "        load_best_model_at_end (`bool`, *optional*, defaults to `False`):\n",
      "            Whether or not to load the best model found during training at the end of training. When this option is\n",
      "            enabled, the best checkpoint will always be saved. See\n",
      "            [`save_total_limit`](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments.save_total_limit)\n",
      "            for more.\n",
      "\n",
      "            <Tip>\n",
      "\n",
      "            When set to `True`, the parameters `save_strategy` needs to be the same as `eval_strategy`, and in\n",
      "            the case it is \"steps\", `save_steps` must be a round multiple of `eval_steps`.\n",
      "\n",
      "            </Tip>\n",
      "\n",
      "        metric_for_best_model (`str`, *optional*):\n",
      "            Use in conjunction with `load_best_model_at_end` to specify the metric to use to compare two different\n",
      "            models. Must be the name of a metric returned by the evaluation with or without the prefix `\"eval_\"`.\n",
      "\n",
      "            If not specified, this will default to `\"loss\"` when either `load_best_model_at_end == True`\n",
      "            or `lr_scheduler_type == SchedulerType.REDUCE_ON_PLATEAU` (to use the evaluation loss).\n",
      "\n",
      "            If you set this value, `greater_is_better` will default to `True` unless the name ends with \"loss\".\n",
      "            Don't forget to set it to `False` if your metric is better when lower.\n",
      "        greater_is_better (`bool`, *optional*):\n",
      "            Use in conjunction with `load_best_model_at_end` and `metric_for_best_model` to specify if better models\n",
      "            should have a greater metric or not. Will default to:\n",
      "\n",
      "            - `True` if `metric_for_best_model` is set to a value that doesn't end in `\"loss\"`.\n",
      "            - `False` if `metric_for_best_model` is not set, or set to a value that ends in `\"loss\"`.\n",
      "        ignore_data_skip (`bool`, *optional*, defaults to `False`):\n",
      "            When resuming training, whether or not to skip the epochs and batches to get the data loading at the same\n",
      "            stage as in the previous training. If set to `True`, the training will begin faster (as that skipping step\n",
      "            can take a long time) but will not yield the same results as the interrupted training would have.\n",
      "        fsdp (`bool`, `str` or list of [`~trainer_utils.FSDPOption`], *optional*, defaults to `[]`):\n",
      "            Use PyTorch Distributed Parallel Training (in distributed training only).\n",
      "\n",
      "            A list of options along the following:\n",
      "\n",
      "            - `\"full_shard\"`: Shard parameters, gradients and optimizer states.\n",
      "            - `\"shard_grad_op\"`: Shard optimizer states and gradients.\n",
      "            - `\"hybrid_shard\"`: Apply `FULL_SHARD` within a node, and replicate parameters across nodes.\n",
      "            - `\"hybrid_shard_zero2\"`: Apply `SHARD_GRAD_OP` within a node, and replicate parameters across nodes.\n",
      "            - `\"offload\"`: Offload parameters and gradients to CPUs (only compatible with `\"full_shard\"` and\n",
      "              `\"shard_grad_op\"`).\n",
      "            - `\"auto_wrap\"`: Automatically recursively wrap layers with FSDP using `default_auto_wrap_policy`.\n",
      "        fsdp_config (`str` or `dict`, *optional*):\n",
      "            Config to be used with fsdp (Pytorch Distributed Parallel Training). The value is either a location of\n",
      "            fsdp json config file (e.g., `fsdp_config.json`) or an already loaded json file as `dict`.\n",
      "\n",
      "            A List of config and its options:\n",
      "                - min_num_params (`int`, *optional*, defaults to `0`):\n",
      "                    FSDP's minimum number of parameters for Default Auto Wrapping. (useful only when `fsdp` field is\n",
      "                    passed).\n",
      "                - transformer_layer_cls_to_wrap (`list[str]`, *optional*):\n",
      "                    List of transformer layer class names (case-sensitive) to wrap, e.g, `BertLayer`, `GPTJBlock`,\n",
      "                    `T5Block` .... (useful only when `fsdp` flag is passed).\n",
      "                - backward_prefetch (`str`, *optional*)\n",
      "                    FSDP's backward prefetch mode. Controls when to prefetch next set of parameters (useful only when\n",
      "                    `fsdp` field is passed).\n",
      "\n",
      "                    A list of options along the following:\n",
      "\n",
      "                    - `\"backward_pre\"` : Prefetches the next set of parameters before the current set of parameter's\n",
      "                      gradient computation.\n",
      "                    - `\"backward_post\"` : This prefetches the next set of parameters after the current set of\n",
      "                      parameter's gradient computation.\n",
      "                - forward_prefetch (`bool`, *optional*, defaults to `False`)\n",
      "                    FSDP's forward prefetch mode (useful only when `fsdp` field is passed).\n",
      "                     If `\"True\"`, then FSDP explicitly prefetches the next upcoming all-gather while executing in the\n",
      "                     forward pass.\n",
      "                - limit_all_gathers (`bool`, *optional*, defaults to `False`)\n",
      "                    FSDP's limit_all_gathers (useful only when `fsdp` field is passed).\n",
      "                     If `\"True\"`, FSDP explicitly synchronizes the CPU thread to prevent too many in-flight\n",
      "                     all-gathers.\n",
      "                - use_orig_params (`bool`, *optional*, defaults to `True`)\n",
      "                    If `\"True\"`, allows non-uniform `requires_grad` during init, which means support for interspersed\n",
      "                    frozen and trainable parameters. Useful in cases such as parameter-efficient fine-tuning. Please\n",
      "                    refer this\n",
      "                    [blog](https://dev-discuss.pytorch.org/t/rethinking-pytorch-fully-sharded-data-parallel-fsdp-from-first-principles/1019\n",
      "                - sync_module_states (`bool`, *optional*, defaults to `True`)\n",
      "                    If `\"True\"`, each individually wrapped FSDP unit will broadcast module parameters from rank 0 to\n",
      "                    ensure they are the same across all ranks after initialization\n",
      "                - cpu_ram_efficient_loading (`bool`, *optional*, defaults to `False`)\n",
      "                    If `\"True\"`, only the first process loads the pretrained model checkpoint while all other processes\n",
      "                    have empty weights.  When this setting as `\"True\"`, `sync_module_states` also must to be `\"True\"`,\n",
      "                    otherwise all the processes except the main process would have random weights leading to unexpected\n",
      "                    behaviour during training.\n",
      "                - activation_checkpointing (`bool`, *optional*, defaults to `False`):\n",
      "                    If `\"True\"`, activation checkpointing is a technique to reduce memory usage by clearing activations of\n",
      "                    certain layers and recomputing them during a backward pass. Effectively, this trades extra\n",
      "                    computation time for reduced memory usage.\n",
      "                - xla (`bool`, *optional*, defaults to `False`):\n",
      "                    Whether to use PyTorch/XLA Fully Sharded Data Parallel Training. This is an experimental feature\n",
      "                    and its API may evolve in the future.\n",
      "                - xla_fsdp_settings (`dict`, *optional*)\n",
      "                    The value is a dictionary which stores the XLA FSDP wrapping parameters.\n",
      "\n",
      "                    For a complete list of options, please see [here](\n",
      "                    https://github.com/pytorch/xla/blob/master/torch_xla/distributed/fsdp/xla_fully_sharded_data_parallel.py).\n",
      "                - xla_fsdp_grad_ckpt (`bool`, *optional*, defaults to `False`):\n",
      "                    Will use gradient checkpointing over each nested XLA FSDP wrapped layer. This setting can only be\n",
      "                    used when the xla flag is set to true, and an auto wrapping policy is specified through\n",
      "                    fsdp_min_num_params or fsdp_transformer_layer_cls_to_wrap.\n",
      "        deepspeed (`str` or `dict`, *optional*):\n",
      "            Use [Deepspeed](https://github.com/deepspeedai/DeepSpeed). This is an experimental feature and its API may\n",
      "            evolve in the future. The value is either the location of DeepSpeed json config file (e.g.,\n",
      "            `ds_config.json`) or an already loaded json file as a `dict`\"\n",
      "\n",
      "            <Tip warning={true}>\n",
      "                If enabling any Zero-init, make sure that your model is not initialized until\n",
      "                *after* initializing the `TrainingArguments`, else it will not be applied.\n",
      "            </Tip>\n",
      "\n",
      "        accelerator_config (`str`, `dict`, or `AcceleratorConfig`, *optional*):\n",
      "            Config to be used with the internal `Accelerator` implementation. The value is either a location of\n",
      "            accelerator json config file (e.g., `accelerator_config.json`), an already loaded json file as `dict`,\n",
      "            or an instance of [`~trainer_pt_utils.AcceleratorConfig`].\n",
      "\n",
      "            A list of config and its options:\n",
      "                - split_batches (`bool`, *optional*, defaults to `False`):\n",
      "                    Whether or not the accelerator should split the batches yielded by the dataloaders across the devices. If\n",
      "                    `True` the actual batch size used will be the same on any kind of distributed processes, but it must be a\n",
      "                    round multiple of the `num_processes` you are using. If `False`, actual batch size used will be the one set\n",
      "                    in your script multiplied by the number of processes.\n",
      "                - dispatch_batches (`bool`, *optional*):\n",
      "                    If set to `True`, the dataloader prepared by the Accelerator is only iterated through on the main process\n",
      "                    and then the batches are split and broadcast to each process. Will default to `True` for `DataLoader` whose\n",
      "                    underlying dataset is an `IterableDataset`, `False` otherwise.\n",
      "                - even_batches (`bool`, *optional*, defaults to `True`):\n",
      "                    If set to `True`, in cases where the total batch size across all processes does not exactly divide the\n",
      "                    dataset, samples at the start of the dataset will be duplicated so the batch can be divided equally among\n",
      "                    all workers.\n",
      "                - use_seedable_sampler (`bool`, *optional*, defaults to `True`):\n",
      "                    Whether or not use a fully seedable random sampler ([`accelerate.data_loader.SeedableRandomSampler`]). Ensures\n",
      "                    training results are fully reproducible using a different sampling technique. While seed-to-seed results\n",
      "                    may differ, on average the differences are negligible when using multiple different seeds to compare. Should\n",
      "                    also be ran with [`~utils.set_seed`] for the best results.\n",
      "                - use_configured_state (`bool`, *optional*, defaults to `False`):\n",
      "                    Whether or not to use a pre-configured `AcceleratorState` or `PartialState` defined before calling `TrainingArguments`.\n",
      "                    If `True`, an `Accelerator` or `PartialState` must be initialized. Note that by doing so, this could lead to issues\n",
      "                    with hyperparameter tuning.\n",
      "        parallelism_config (`ParallelismConfig`, *optional*):\n",
      "            Parallelism configuration for the training run. Requires Accelerate `1.10.1`\n",
      "        label_smoothing_factor (`float`, *optional*, defaults to 0.0):\n",
      "            The label smoothing factor to use. Zero means no label smoothing, otherwise the underlying onehot-encoded\n",
      "            labels are changed from 0s and 1s to `label_smoothing_factor/num_labels` and `1 - label_smoothing_factor +\n",
      "            label_smoothing_factor/num_labels` respectively.\n",
      "        debug (`str` or list of [`~debug_utils.DebugOption`], *optional*, defaults to `\"\"`):\n",
      "            Enable one or more debug features. This is an experimental feature.\n",
      "\n",
      "            Possible options are:\n",
      "\n",
      "            - `\"underflow_overflow\"`: detects overflow in model's input/outputs and reports the last frames that led to\n",
      "              the event\n",
      "            - `\"tpu_metrics_debug\"`: print debug metrics on TPU\n",
      "\n",
      "            The options should be separated by whitespaces.\n",
      "        optim (`str` or [`training_args.OptimizerNames`], *optional*, defaults to `\"adamw_torch\"` (for torch>=2.8 `\"adamw_torch_fused\"`)):\n",
      "            The optimizer to use, such as \"adamw_torch\", \"adamw_torch_fused\", \"adamw_apex_fused\", \"adamw_anyprecision\",\n",
      "            \"adafactor\". See `OptimizerNames` in [training_args.py](https://github.com/huggingface/transformers/blob/main/src/transformers/training_args.py)\n",
      "            for a full list of optimizers.\n",
      "        optim_args (`str`, *optional*):\n",
      "            Optional arguments that are supplied to optimizers such as AnyPrecisionAdamW, AdEMAMix, and GaLore.\n",
      "        group_by_length (`bool`, *optional*, defaults to `False`):\n",
      "            Whether or not to group together samples of roughly the same length in the training dataset (to minimize\n",
      "            padding applied and be more efficient). Only useful if applying dynamic padding.\n",
      "        length_column_name (`str`, *optional*, defaults to `\"length\"`):\n",
      "            Column name for precomputed lengths. If the column exists, grouping by length will use these values rather\n",
      "            than computing them on train startup. Ignored unless `group_by_length` is `True` and the dataset is an\n",
      "            instance of `Dataset`.\n",
      "        report_to (`str` or `list[str]`, *optional*, defaults to `\"all\"`):\n",
      "            The list of integrations to report the results and logs to. Supported platforms are `\"azure_ml\"`,\n",
      "            `\"clearml\"`, `\"codecarbon\"`, `\"comet_ml\"`, `\"dagshub\"`, `\"dvclive\"`, `\"flyte\"`, `\"mlflow\"`, `\"neptune\"`,\n",
      "            `\"swanlab\"`, `\"tensorboard\"`, `\"trackio\"` and `\"wandb\"`. Use `\"all\"` to report to all integrations\n",
      "            installed, `\"none\"` for no integrations.\n",
      "        project (`str`, *optional*, defaults to `\"huggingface\"`):\n",
      "            The name of the project to use for logging. Currently, only used by Trackio.\n",
      "        trackio_space_id (`str` or `None`, *optional*, defaults to `\"trackio\"`):\n",
      "            The Hugging Face Space ID to deploy to when using Trackio. Should be a complete Space name like\n",
      "            `'username/reponame'` or `'orgname/reponame' `, or just `'reponame'` in which case the Space will be\n",
      "            created in the currently-logged-in Hugging Face user's namespace. If `None`, will log to a local directory.\n",
      "            Note that this Space will be public unless you set `hub_private_repo=True` or your organization's default\n",
      "            is to create private Spaces.\"\n",
      "        ddp_find_unused_parameters (`bool`, *optional*):\n",
      "            When using distributed training, the value of the flag `find_unused_parameters` passed to\n",
      "            `DistributedDataParallel`. Will default to `False` if gradient checkpointing is used, `True` otherwise.\n",
      "        ddp_bucket_cap_mb (`int`, *optional*):\n",
      "            When using distributed training, the value of the flag `bucket_cap_mb` passed to `DistributedDataParallel`.\n",
      "        ddp_broadcast_buffers (`bool`, *optional*):\n",
      "            When using distributed training, the value of the flag `broadcast_buffers` passed to\n",
      "            `DistributedDataParallel`. Will default to `False` if gradient checkpointing is used, `True` otherwise.\n",
      "        dataloader_pin_memory (`bool`, *optional*, defaults to `True`):\n",
      "            Whether you want to pin memory in data loaders or not. Will default to `True`.\n",
      "        dataloader_persistent_workers (`bool`, *optional*, defaults to `False`):\n",
      "            If True, the data loader will not shut down the worker processes after a dataset has been consumed once.\n",
      "            This allows to maintain the workers Dataset instances alive. Can potentially speed up training, but will\n",
      "            increase RAM usage. Will default to `False`.\n",
      "        dataloader_prefetch_factor (`int`, *optional*):\n",
      "            Number of batches loaded in advance by each worker.\n",
      "            2 means there will be a total of 2 * num_workers batches prefetched across all workers.\n",
      "        skip_memory_metrics (`bool`, *optional*, defaults to `True`):\n",
      "            Whether to skip adding of memory profiler reports to metrics. This is skipped by default because it slows\n",
      "            down the training and evaluation speed.\n",
      "        push_to_hub (`bool`, *optional*, defaults to `False`):\n",
      "            Whether or not to push the model to the Hub every time the model is saved. If this is activated,\n",
      "            `output_dir` will begin a git directory synced with the repo (determined by `hub_model_id`) and the content\n",
      "            will be pushed each time a save is triggered (depending on your `save_strategy`). Calling\n",
      "            [`~Trainer.save_model`] will also trigger a push.\n",
      "\n",
      "            <Tip warning={true}>\n",
      "\n",
      "            If `output_dir` exists, it needs to be a local clone of the repository to which the [`Trainer`] will be\n",
      "            pushed.\n",
      "\n",
      "            </Tip>\n",
      "\n",
      "        resume_from_checkpoint (`str`, *optional*):\n",
      "            The path to a folder with a valid checkpoint for your model. This argument is not directly used by\n",
      "            [`Trainer`], it's intended to be used by your training/evaluation scripts instead. See the [example\n",
      "            scripts](https://github.com/huggingface/transformers/tree/main/examples) for more details.\n",
      "        hub_model_id (`str`, *optional*):\n",
      "            The name of the repository to keep in sync with the local *output_dir*. It can be a simple model ID in\n",
      "            which case the model will be pushed in your namespace. Otherwise it should be the whole repository name,\n",
      "            for instance `\"user_name/model\"`, which allows you to push to an organization you are a member of with\n",
      "            `\"organization_name/model\"`. Will default to `user_name/output_dir_name` with *output_dir_name* being the\n",
      "            name of `output_dir`.\n",
      "\n",
      "            Will default to the name of `output_dir`.\n",
      "        hub_strategy (`str` or [`~trainer_utils.HubStrategy`], *optional*, defaults to `\"every_save\"`):\n",
      "            Defines the scope of what is pushed to the Hub and when. Possible values are:\n",
      "\n",
      "            - `\"end\"`: push the model, its configuration, the processing class e.g. tokenizer (if passed along to the [`Trainer`]) and a\n",
      "              draft of a model card when the [`~Trainer.save_model`] method is called.\n",
      "            - `\"every_save\"`: push the model, its configuration, the processing class e.g. tokenizer (if passed along to the [`Trainer`]) and\n",
      "              a draft of a model card each time there is a model save. The pushes are asynchronous to not block\n",
      "              training, and in case the save are very frequent, a new push is only attempted if the previous one is\n",
      "              finished. A last push is made with the final model at the end of training.\n",
      "            - `\"checkpoint\"`: like `\"every_save\"` but the latest checkpoint is also pushed in a subfolder named\n",
      "              last-checkpoint, allowing you to resume training easily with\n",
      "              `trainer.train(resume_from_checkpoint=\"last-checkpoint\")`.\n",
      "            - `\"all_checkpoints\"`: like `\"checkpoint\"` but all checkpoints are pushed like they appear in the output\n",
      "              folder (so you will get one checkpoint folder per folder in your final repository)\n",
      "\n",
      "        hub_token (`str`, *optional*):\n",
      "            The token to use to push the model to the Hub. Will default to the token in the cache folder obtained with\n",
      "            `hf auth login`.\n",
      "        hub_private_repo (`bool`, *optional*):\n",
      "            Whether to make the repo private. If `None` (default), the repo will be public unless the organization's\n",
      "            default is private. This value is ignored if the repo already exists. If reporting to Trackio with\n",
      "            deployment to Hugging Face Spaces enabled, the same logic determines whether the Space is private.\n",
      "        hub_always_push (`bool`, *optional*, defaults to `False`):\n",
      "            Unless this is `True`, the `Trainer` will skip pushing a checkpoint when the previous push is not finished.\n",
      "        hub_revision (`str`, *optional*):\n",
      "            The revision to use when pushing to the Hub. Can be a branch name, a tag, or a commit hash.\n",
      "        gradient_checkpointing (`bool`, *optional*, defaults to `False`):\n",
      "            If True, use gradient checkpointing to save memory at the expense of slower backward pass.\n",
      "        gradient_checkpointing_kwargs (`dict`, *optional*, defaults to `None`):\n",
      "            Key word arguments to be passed to the `gradient_checkpointing_enable` method.\n",
      "        include_inputs_for_metrics (`bool`, *optional*, defaults to `False`):\n",
      "            This argument is deprecated. Use `include_for_metrics` instead, e.g, `include_for_metrics = [\"inputs\"]`.\n",
      "        include_for_metrics (`list[str]`, *optional*, defaults to `[]`):\n",
      "            Include additional data in the `compute_metrics` function if needed for metrics computation.\n",
      "            Possible options to add to `include_for_metrics` list:\n",
      "            - `\"inputs\"`: Input data passed to the model, intended for calculating input dependent metrics.\n",
      "            - `\"loss\"`: Loss values computed during evaluation, intended for calculating loss dependent metrics.\n",
      "        eval_do_concat_batches (`bool`, *optional*, defaults to `True`):\n",
      "            Whether to recursively concat inputs/losses/labels/predictions across batches. If `False`,\n",
      "            will instead store them as lists, with each batch kept separate.\n",
      "        auto_find_batch_size (`bool`, *optional*, defaults to `False`)\n",
      "            Whether to find a batch size that will fit into memory automatically through exponential decay, avoiding\n",
      "            CUDA Out-of-Memory errors. Requires accelerate to be installed (`pip install accelerate`)\n",
      "        full_determinism (`bool`, *optional*, defaults to `False`)\n",
      "            If `True`, [`enable_full_determinism`] is called instead of [`set_seed`] to ensure reproducible results in\n",
      "            distributed training. Important: this will negatively impact the performance, so only use it for debugging.\n",
      "        torchdynamo (`str`, *optional*):\n",
      "            If set, the backend compiler for TorchDynamo. Possible choices are `\"eager\"`, `\"aot_eager\"`, `\"inductor\"`,\n",
      "            `\"nvfuser\"`, `\"aot_nvfuser\"`, `\"aot_cudagraphs\"`, `\"ofi\"`, `\"fx2trt\"`, `\"onnxrt\"` and `\"ipex\"`.\n",
      "        ray_scope (`str`, *optional*, defaults to `\"last\"`):\n",
      "            The scope to use when doing hyperparameter search with Ray. By default, `\"last\"` will be used. Ray will\n",
      "            then use the last checkpoint of all trials, compare those, and select the best one. However, other options\n",
      "            are also available. See the [Ray documentation](\n",
      "            https://docs.ray.io/en/latest/tune/api_docs/analysis.html#ray.tune.ExperimentAnalysis.get_best_trial) for\n",
      "            more options.\n",
      "        ddp_timeout (`int`, *optional*, defaults to 1800):\n",
      "            The timeout for `torch.distributed.init_process_group` calls, used to avoid GPU socket timeouts when\n",
      "            performing slow operations in distributed runnings. Please refer the [PyTorch documentation]\n",
      "            (https://pytorch.org/docs/stable/distributed.html#torch.distributed.init_process_group) for more\n",
      "            information.\n",
      "        use_mps_device (`bool`, *optional*, defaults to `False`):\n",
      "            This argument is deprecated.`mps` device will be used if it is available similar to `cuda` device.\n",
      "        torch_compile (`bool`, *optional*, defaults to `False`):\n",
      "            Whether or not to compile the model using PyTorch 2.0\n",
      "            [`torch.compile`](https://pytorch.org/get-started/pytorch-2.0/).\n",
      "\n",
      "            This will use the best defaults for the [`torch.compile`\n",
      "            API](https://pytorch.org/docs/stable/generated/torch.compile.html?highlight=torch+compile#torch.compile).\n",
      "            You can customize the defaults with the argument `torch_compile_backend` and `torch_compile_mode` but we\n",
      "            don't guarantee any of them will work as the support is progressively rolled in in PyTorch.\n",
      "\n",
      "            This flag and the whole compile API is experimental and subject to change in future releases.\n",
      "        torch_compile_backend (`str`, *optional*):\n",
      "            The backend to use in `torch.compile`. If set to any value, `torch_compile` will be set to `True`.\n",
      "\n",
      "            Refer to the PyTorch doc for possible values and note that they may change across PyTorch versions.\n",
      "\n",
      "            This flag is experimental and subject to change in future releases.\n",
      "        torch_compile_mode (`str`, *optional*):\n",
      "            The mode to use in `torch.compile`. If set to any value, `torch_compile` will be set to `True`.\n",
      "\n",
      "            Refer to the PyTorch doc for possible values and note that they may change across PyTorch versions.\n",
      "\n",
      "            This flag is experimental and subject to change in future releases.\n",
      "        include_tokens_per_second (`bool`, *optional*, defaults to `False`):\n",
      "            Whether or not to compute the number of tokens per second per device for training speed metrics.\n",
      "\n",
      "            This will iterate over the entire training dataloader once beforehand,\n",
      "            and will slow down the entire process.\n",
      "\n",
      "        include_num_input_tokens_seen (`bool`, *optional*):\n",
      "            Whether or not to track the number of input tokens seen throughout training.\n",
      "\n",
      "            May be slower in distributed training as gather operations must be called.\n",
      "\n",
      "        neftune_noise_alpha (`Optional[float]`):\n",
      "            If not `None`, this will activate NEFTune noise embeddings. This can drastically improve model performance\n",
      "            for instruction fine-tuning. Check out the [original paper](https://huggingface.co/papers/2310.05914) and the\n",
      "            [original code](https://github.com/neelsjain/NEFTune). Support transformers `PreTrainedModel` and also\n",
      "            `PeftModel` from peft. The original paper used values in the range [5.0, 15.0].\n",
      "        optim_target_modules (`Union[str, list[str]]`, *optional*):\n",
      "            The target modules to optimize, i.e. the module names that you would like to train.\n",
      "            Currently used for the GaLore algorithm (https://huggingface.co/papers/2403.03507) and APOLLO algorithm (https://huggingface.co/papers/2412.05270).\n",
      "            See GaLore implementation (https://github.com/jiaweizzhao/GaLore) and APOLLO implementation (https://github.com/zhuhanqing/APOLLO) for more details.\n",
      "            You need to make sure to pass a valid GaLore or APOLLO optimizer, e.g., one of: \"apollo_adamw\", \"galore_adamw\", \"galore_adamw_8bit\", \"galore_adafactor\" and make sure that the target modules are `nn.Linear` modules only.\n",
      "\n",
      "        batch_eval_metrics (`bool`, *optional*, defaults to `False`):\n",
      "            If set to `True`, evaluation will call compute_metrics at the end of each batch to accumulate statistics\n",
      "            rather than saving all eval logits in memory. When set to `True`, you must pass a compute_metrics function\n",
      "            that takes a boolean argument `compute_result`, which when passed `True`, will trigger the final global\n",
      "            summary statistics from the batch-level summary statistics you've accumulated over the evaluation set.\n",
      "\n",
      "        eval_on_start (`bool`, *optional*, defaults to `False`):\n",
      "            Whether to perform a evaluation step (sanity check) before the training to ensure the validation steps works correctly.\n",
      "\n",
      "        eval_use_gather_object (`bool`, *optional*, defaults to `False`):\n",
      "            Whether to run recursively gather object in a nested list/tuple/dictionary of objects from all devices. This should only be enabled if users are not just returning tensors, and this is actively discouraged by PyTorch.\n",
      "\n",
      "        use_liger_kernel (`bool`, *optional*, defaults to `False`):\n",
      "            Whether enable [Liger](https://github.com/linkedin/Liger-Kernel) Kernel for LLM model training.\n",
      "            It can effectively increase multi-GPU training throughput by ~20% and reduces memory usage by ~60%, works out of the box with\n",
      "            flash attention, PyTorch FSDP, and Microsoft DeepSpeed. Currently, it supports llama, mistral, mixtral and gemma models.\n",
      "\n",
      "        liger_kernel_config (`Optional[dict]`, *optional*):\n",
      "            Configuration to be used for Liger Kernel. When use_liger_kernel=True, this dict is passed as keyword arguments to the\n",
      "            `_apply_liger_kernel_to_instance` function, which specifies which kernels to apply. Available options vary by model but typically\n",
      "            include: 'rope', 'swiglu', 'cross_entropy', 'fused_linear_cross_entropy', 'rms_norm', etc. If `None`, use the default kernel configurations.\n",
      "\n",
      "        average_tokens_across_devices (`bool`, *optional*, defaults to `True`):\n",
      "            Whether or not to average tokens across devices. If enabled, will use all_reduce to synchronize\n",
      "            num_tokens_in_batch for precise loss calculation. Reference:\n",
      "            https://github.com/huggingface/transformers/issues/34242\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "print(TrainingArguments.__doc__)  # Should show evaluation_strategy and save_strategy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b95b2187-60c9-4a03-9aae-1866398d8324",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "848e90f2-cecd-4c14-b876-bec65bed6d97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['tweet_id', 'text', 'label'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv(r\"F:\\pbl research\\new research paper\\Sarcasm_combined.csv\")\n",
    "\n",
    "# Inspect columns\n",
    "print(df.columns)\n",
    "\n",
    "# Example: if columns are ['text', 'label'], adjust below accordingly\n",
    "text_column = 'text'    # replace if different\n",
    "label_column = 'label'  # replace if different\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "083da7cd-2a5e-41e8-9713-a819ff93194d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Remove unwanted characters, keep letters and spaces\n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", str(text))\n",
    "    return text.lower()\n",
    "\n",
    "df['clean_text'] = df[text_column].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14b13505-37cb-4088-ae8a-8f13048b93a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode labels as integers\n",
    "le = LabelEncoder()\n",
    "df['label_encoded'] = le.fit_transform(df[label_column])\n",
    "\n",
    "# Split train/test\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    df['clean_text'], df['label_encoded'], test_size=0.2, random_state=42, stratify=df['label_encoded']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "405f2d40-1ef7-423d-a96b-6c0b35f80150",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bert-base-multilingual-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Tokenization\n",
    "train_encodings = tokenizer(list(train_texts), truncation=True, padding=True, max_length=128)\n",
    "test_encodings = tokenizer(list(test_texts), truncation=True, padding=True, max_length=128)\n",
    "\n",
    "# Convert to Hugging Face Dataset\n",
    "train_dataset = Dataset.from_dict({\n",
    "    'input_ids': train_encodings['input_ids'],\n",
    "    'attention_mask': train_encodings['attention_mask'],\n",
    "    'labels': list(train_labels)\n",
    "})\n",
    "\n",
    "test_dataset = Dataset.from_dict({\n",
    "    'input_ids': test_encodings['input_ids'],\n",
    "    'attention_mask': test_encodings['attention_mask'],\n",
    "    'labels': list(test_labels)\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ba9fc99-002b-4034-b8bf-16701d0cd992",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(le.classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b421345-f382-4678-a16f-9051f3d9c93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./bert_sarcasm\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e100b56-6098-4ebf-ab71-e3d2ab45e563",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "    return {\n",
    "        'accuracy': accuracy_score(labels, preds),\n",
    "        'precision': precision_score(labels, preds, zero_division=0),\n",
    "        'recall': recall_score(labels, preds, zero_division=0),\n",
    "        'f1': f1_score(labels, preds, average='binary')\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a877c77b-4efc-4958-8d98-4a88d8a8fb5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Charvi\\AppData\\Local\\Temp\\ipykernel_41816\\1271713805.py:2: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "67ab2d7a-d973-43f7-a4af-2da300b99c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Charvi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='789' max='789' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [789/789 1:48:02, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.104550</td>\n",
       "      <td>0.955238</td>\n",
       "      <td>0.695652</td>\n",
       "      <td>0.950495</td>\n",
       "      <td>0.803347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.128600</td>\n",
       "      <td>0.085544</td>\n",
       "      <td>0.959048</td>\n",
       "      <td>0.707143</td>\n",
       "      <td>0.980198</td>\n",
       "      <td>0.821577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.128600</td>\n",
       "      <td>0.078887</td>\n",
       "      <td>0.968571</td>\n",
       "      <td>0.793103</td>\n",
       "      <td>0.910891</td>\n",
       "      <td>0.847926</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Charvi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\Charvi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=789, training_loss=0.11480729570860192, metrics={'train_runtime': 6491.4108, 'train_samples_per_second': 1.941, 'train_steps_per_second': 0.122, 'total_flos': 401449914936000.0, 'train_loss': 0.11480729570860192, 'epoch': 3.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 11: Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "94394778-b944-4596-ae78-fc9a1d565753",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Charvi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='33' max='33' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [33/33 01:28]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.07888709753751755,\n",
       " 'eval_accuracy': 0.9685714285714285,\n",
       " 'eval_precision': 0.7931034482758621,\n",
       " 'eval_recall': 0.9108910891089109,\n",
       " 'eval_f1': 0.847926267281106,\n",
       " 'eval_runtime': 91.0784,\n",
       " 'eval_samples_per_second': 11.529,\n",
       " 'eval_steps_per_second': 0.362,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 12: Evaluate\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa4f3a0-c4b2-4656-b2ff-9daa447fc440",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
